"""
æµ‹è¯• NLP Service çš„ç« èŠ‚æ‹†åˆ†åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.services.parse_service import parse_markdown_file
from app.services.nlp_service import extract_and_split_sections, get_segments_summary
import json


def test_section_splitting():
    """æµ‹è¯•ç« èŠ‚æ‹†åˆ†åŠŸèƒ½"""
    
    print("=" * 80)
    print("NLP Service - ç« èŠ‚æ‹†åˆ†åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    print()
    
    # æ–¹å¼1: ä½¿ç”¨å®é™…çš„ Markdown æ–‡ä»¶
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹æ–‡ä»¶è·¯å¾„
    test_md_files = [
        r"D:\MyFiles\AIPPT\Code\keenPoint\downloads\Lin_HRank_Filter_Pruning_Using_High-Rank_Feature_Map_CVPR_2020_paper\full.md"
    ]
    
    md_file = None
    for test_file in test_md_files:
        if Path(test_file).exists():
            md_file = test_file
            break
    
    if md_file:
        print(f"ğŸ“„ ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {md_file}")
        print()
        
        # 1. è§£æ Markdown æ–‡ä»¶
        print("æ­¥éª¤ 1: è§£æ Markdown æ–‡ä»¶...")
        parse_result = parse_markdown_file(md_file)
        
        sections = parse_result.get("sections", [])
        print(f"âœ… è§£æå®Œæˆï¼Œå…± {len(sections)} ä¸ªç« èŠ‚")
        print()
        
        # 2. æå–å¹¶æ‹†åˆ†ç« èŠ‚
        print("æ­¥éª¤ 2: æå–å¹¶æ‹†åˆ†ç« èŠ‚...")
        segments = extract_and_split_sections(parse_result)
        print(f"âœ… æ‹†åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
        print()
        
        # 3. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("æ­¥éª¤ 3: ç»Ÿè®¡ä¿¡æ¯")
        print("-" * 80)
        summary = get_segments_summary(segments)
        for key, value in summary.items():
            print(f"  {key}: {value}")
        print()
        
        # 4. æ˜¾ç¤ºå‰å‡ ä¸ªç‰‡æ®µçš„è¯¦ç»†ä¿¡æ¯
        print("æ­¥éª¤ 4: ç‰‡æ®µè¯¦æƒ…ï¼ˆå‰5ä¸ªï¼‰")
        print("-" * 80)
        for i, segment in enumerate(segments[:5], 1):
            print(f"\n[ç‰‡æ®µ {i}]")
            print(f"  ID: {segment['id']}")
            print(f"  åç§°: {segment['name']}")
            print(f"  åŸå§‹ç« èŠ‚ç´¢å¼•: {segment['original_section_index']}")
            print(f"  æ˜¯å¦æ‹†åˆ†: {segment['is_split']}")
            if segment['is_split']:
                print(f"  éƒ¨åˆ†: {segment['part_index']}/{segment['total_parts']}")
            content_preview = segment['content'][:100].replace('\n', ' ')
            print(f"  å†…å®¹é•¿åº¦: {len(segment['content'])} å­—ç¬¦")
            print(f"  å†…å®¹é¢„è§ˆ: {content_preview}...")
        
        if len(segments) > 5:
            print(f"\n... è¿˜æœ‰ {len(segments) - 5} ä¸ªç‰‡æ®µ")
        
        # 5. ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶
        output_file = project_root / "test_segments_output.json"
        print()
        print("æ­¥éª¤ 5: ä¿å­˜ç»“æœåˆ°æ–‡ä»¶")
        print("-" * 80)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": summary,
                "segments": segments
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    try:
        test_section_splitting()
        print()
        print("=" * 80) 
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("=" * 80)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\næŒ‰ä»»æ„é”®é€€å‡º...")
    input()
