"""
NLP æœåŠ¡
æä¾›è‡ªç„¶è¯­è¨€å¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç»“æ„åˆ†æã€å…³é”®è¯æå–å’Œæ‘˜è¦ç”Ÿæˆ
"""

import re
from typing import Dict, List
from collections import Counter
from app.core.logger import logger

def analyze_structure(text: str) -> Dict:
    """
    åˆ†æè®ºæ–‡ç»“æ„ - è¯†åˆ«ç« èŠ‚
    
    ä½¿ç”¨å…³é”®è¯åŒ¹é…æ–¹æ³•è¯†åˆ«å­¦æœ¯è®ºæ–‡çš„æ ‡å‡†ç« èŠ‚
    
    å‚æ•°:
    - text: è®ºæ–‡æ–‡æœ¬å†…å®¹
    
    è¿”å›:
    - ç»“æ„åˆ†æç»“æœ
    """
    logger.info("ğŸ” å¼€å§‹åˆ†æè®ºæ–‡ç»“æ„...")
    
    # æ ‡å‡†å­¦æœ¯è®ºæ–‡ç« èŠ‚å…³é”®è¯
    section_keywords = {
        "Abstract": ["abstract", "æ‘˜è¦"],
        "Introduction": ["introduction", "å¼•è¨€", "ç»ªè®º"],
        "Related Work": ["related work", "literature review", "ç›¸å…³å·¥ä½œ", "æ–‡çŒ®ç»¼è¿°"],
        "Methodology": ["methodology", "methods", "approach", "æ–¹æ³•", "æ–¹æ³•è®º"],
        "Experiment": ["experiment", "experimental", "å®éªŒ"],
        "Results": ["results", "ç»“æœ"],
        "Discussion": ["discussion", "è®¨è®º"],
        "Conclusion": ["conclusion", "conclusions", "ç»“è®º"],
        "References": ["references", "bibliography", "å‚è€ƒæ–‡çŒ®"]
    }
    
    # è½¬ä¸ºå°å†™ä¾¿äºåŒ¹é…
    text_lower = text.lower()
    
    # æ£€æµ‹åˆ°çš„ç« èŠ‚
    detected_sections = []
    section_details = {}
    
    for section_name, keywords in section_keywords.items():
        for keyword in keywords:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
            pattern = rf'\b{re.escape(keyword)}\b'
            matches = list(re.finditer(pattern, text_lower))
            
            if matches:
                detected_sections.append(section_name)
                section_details[section_name] = {
                    "keyword_matched": keyword,
                    "occurrences": len(matches),
                    "first_position": matches[0].start()
                }
                break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è·³å‡º
    
    # æŒ‰ç…§åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„ä½ç½®æ’åº
    detected_sections = sorted(
        detected_sections,
        key=lambda x: section_details[x]["first_position"]
    )
    
    result = {
        "sections_detected": detected_sections,
        "section_count": len(detected_sections),
        "details": section_details
    }
    
    logger.info(f"âœ… ç»“æ„åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(detected_sections)} ä¸ªç« èŠ‚: {', '.join(detected_sections)}")
    
    return result

def extract_keywords(text: str, top_n: int = 10) -> List[str]:
    """
    æå–å…³é”®è¯
    
    ä½¿ç”¨ç®€å•çš„è¯é¢‘ç»Ÿè®¡æ–¹æ³•æå–å…³é”®è¯
    (å®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨ TF-IDF æˆ–æ›´å¤æ‚çš„ç®—æ³•)
    
    å‚æ•°:
    - text: æ–‡æœ¬å†…å®¹
    - top_n: è¿”å›å‰ N ä¸ªå…³é”®è¯
    
    è¿”å›:
    - å…³é”®è¯åˆ—è¡¨
    """
    logger.info(f"ğŸ”‘ å¼€å§‹æå–å…³é”®è¯ (Top {top_n})...")
    
    # æ¸…ç†æ–‡æœ¬
    text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
    
    # åœç”¨è¯åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    stop_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
        'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
        'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',
        'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which',
        'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both',
        'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',
        'only', 'own', 'same', 'so', 'than', 'too', 'very'
    }
    
    # åˆ†è¯
    words = text_clean.split()
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    filtered_words = [
        word for word in words
        if word not in stop_words and len(word) > 3
    ]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(filtered_words)
    
    # è·å–æœ€å¸¸è§çš„è¯
    keywords = [word for word, freq in word_freq.most_common(top_n)]
    
    logger.info(f"âœ… å…³é”®è¯æå–å®Œæˆ: {', '.join(keywords[:5])}...")
    
    return keywords

def generate_summary(text: str, max_length: int = 200) -> str:
    """
    ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
    
    ä½¿ç”¨ç®€å•çš„å¥å­æå–æ–¹æ³•ç”Ÿæˆæ‘˜è¦
    (å®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨ BERT ç­‰é¢„è®­ç»ƒæ¨¡å‹)
    
    å‚æ•°:
    - text: æ–‡æœ¬å†…å®¹
    - max_length: æ‘˜è¦æœ€å¤§é•¿åº¦
    
    è¿”å›:
    - ç”Ÿæˆçš„æ‘˜è¦
    """
    logger.info("ğŸ“ å¼€å§‹ç”Ÿæˆæ‘˜è¦...")
    
    # åˆ†å¥
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    
    if not sentences:
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼šæ–‡æœ¬è¿‡çŸ­ã€‚"
    
    # ç®€å•ç­–ç•¥ï¼šå–å‰å‡ å¥ä½œä¸ºæ‘˜è¦
    summary = ""
    for sentence in sentences[:5]:  # æœ€å¤šå–å‰5å¥
        if len(summary) + len(sentence) > max_length:
            break
        summary += sentence + ". "
    
    if not summary:
        summary = sentences[0][:max_length] + "..."
    
    logger.info(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(summary)}")
    
    return summary.strip()

def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€
    
    ç®€å•çš„è¯­è¨€æ£€æµ‹ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰
    
    å‚æ•°:
    - text: æ–‡æœ¬å†…å®¹
    
    è¿”å›:
    - è¯­è¨€ä»£ç  ('zh' æˆ– 'en')
    """
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    
    # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦æ•°é‡
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    if chinese_chars > english_chars:
        return "zh"
    else:
        return "en"

def extract_citations(text: str) -> List[str]:
    """
    æå–å¼•ç”¨ä¿¡æ¯
    
    å‚æ•°:
    - text: æ–‡æœ¬å†…å®¹
    
    è¿”å›:
    - å¼•ç”¨åˆ—è¡¨
    """
    # ç®€å•çš„å¼•ç”¨æ¨¡å¼åŒ¹é… [1], [2], etc.
    citations = re.findall(r'\[\d+\]', text)
    return list(set(citations))  # å»é‡

def count_figures_and_tables(text: str) -> Dict:
    """
    ç»Ÿè®¡å›¾è¡¨æ•°é‡
    
    å‚æ•°:
    - text: æ–‡æœ¬å†…å®¹
    
    è¿”å›:
    - ç»Ÿè®¡ç»“æœ
    """
    figures = len(re.findall(r'Figure\s+\d+|å›¾\s*\d+', text, re.IGNORECASE))
    tables = len(re.findall(r'Table\s+\d+|è¡¨\s*\d+', text, re.IGNORECASE))
    
    return {
        "figures": figures,
        "tables": tables,
        "total": figures + tables
    }
